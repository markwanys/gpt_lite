{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2258,"status":"ok","timestamp":1690341226011,"user":{"displayName":"Mark Wan","userId":"11243616124484774426"},"user_tz":-480},"id":"do7_XncvZfTD","outputId":"e3af02c5-af9c-4945-bc31-3eeb7b1da9df"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"pW7lYfRFZfTE","executionInfo":{"status":"ok","timestamp":1690341226011,"user_tz":-480,"elapsed":3,"user":{"displayName":"Mark Wan","userId":"11243616124484774426"}}},"outputs":[],"source":["import os\n","os.chdir('/content/drive/MyDrive/Workspaces/gpt_lite')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"uM1d98w9hPDI","executionInfo":{"status":"ok","timestamp":1690341232170,"user_tz":-480,"elapsed":6161,"user":{"displayName":"Mark Wan","userId":"11243616124484774426"}}},"outputs":[],"source":["import torch\n","import torch.optim as optim\n","from utils import *"]},{"cell_type":"markdown","metadata":{"id":"T3oR3T89hPDG"},"source":["# <b> GPT-Lite"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1690341232172,"user":{"displayName":"Mark Wan","userId":"11243616124484774426"},"user_tz":-480},"id":"-BCeFhcNhPDJ","outputId":"d0013842-9855-4bef-e112-e3665df7da2c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded Device:  cuda:0\n"]}],"source":["# Set seed for reproducibility.\n","seed = 1\n","set_seed(seed)\n","\n","# Load GPU.\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print('Loaded Device: ', device)"]},{"cell_type":"markdown","metadata":{"id":"isN6cv1wkKTW"},"source":["### <b> Explore Input"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"f1AHAVaHKkWm","executionInfo":{"status":"ok","timestamp":1690341232713,"user_tz":-480,"elapsed":547,"user":{"displayName":"Mark Wan","userId":"11243616124484774426"}}},"outputs":[],"source":["# Explore input\n","with open('input_shakespeare.txt', 'r', encoding='utf-8') as f:\n","    text = f.read()"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1690341232713,"user":{"displayName":"Mark Wan","userId":"11243616124484774426"},"user_tz":-480},"id":"RDbtE2MI_kk2","outputId":"f2588304-7b73-4fda-a901-77ae418742a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of tokens: 1115394\n"]}],"source":["print(f'Total number of tokens: {len(text)}')"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1690341232713,"user":{"displayName":"Mark Wan","userId":"11243616124484774426"},"user_tz":-480},"id":"xo-NDYfy_kk2","outputId":"26abe367-f7ac-425f-db94-03d11ff42563"},"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You are all resolved rather to die than to famish?\n","\n","All:\n","Resolved. resolved.\n","\n","First Citizen:\n","First, you know Caius Marcius is chief enemy to the people.\n","\n","All:\n","We know't, we know't.\n","\n","First Citizen:\n","Let us kill him, and we'll have corn at our own price.\n","Is't a verdict?\n","\n","All:\n","No more talking on't; let it be done: away, away!\n","\n","Second Citizen:\n","One word, good citizens.\n","\n","First Citizen:\n","We are accounted poor\n"]}],"source":["print(text[:500])"]},{"cell_type":"markdown","metadata":{"id":"WR_ieusZhPDK"},"source":["### <b> Hyperparameters"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"-cj1hL5nhPDK","executionInfo":{"status":"ok","timestamp":1690341232713,"user_tz":-480,"elapsed":5,"user":{"displayName":"Mark Wan","userId":"11243616124484774426"}}},"outputs":[],"source":["# Define data loading parameters.\n","batch_size = 64 # 64 # 16\n","context_size = 256 # 256 # 32\n","max_iterations = 5000\n","eval_interval = 500 # 500 # 100\n","learning_rate = 3e-4 # 3e-4 # 1e-3\n","eval_iters = 200\n","n_embeddings = 384 # 384 # 64\n","n_heads = 6 # 6 # 4\n","n_layers = 6 # 6 # 4\n","dropout = 0.2 # 0.2 # 0.0\n","train_test_split = 0.9"]},{"cell_type":"markdown","metadata":{"id":"OlUIaptp_kk3"},"source":["### <b> Load Data"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"cYs-_KoY_kk4","executionInfo":{"status":"ok","timestamp":1690341233105,"user_tz":-480,"elapsed":395,"user":{"displayName":"Mark Wan","userId":"11243616124484774426"}}},"outputs":[],"source":["vocab_size, encode, decode, train_data, val_data = data(text, train_test_split)"]},{"cell_type":"markdown","metadata":{"id":"YtNjnIrahPDP"},"source":["### <b> Initiate GPT"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5326,"status":"ok","timestamp":1690341238430,"user":{"displayName":"Mark Wan","userId":"11243616124484774426"},"user_tz":-480},"id":"9ojSXwqFhPDP","outputId":"9a2776d5-c061-4608-b28f-679ca1fd494b"},"outputs":[{"output_type":"stream","name":"stdout","text":["10.788929 M parameters\n"]}],"source":["# Initiate GPT.\n","model = BigramModel(vocab_size, n_embeddings, n_heads, n_layers, context_size, dropout, device)\n","model = model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"]},{"cell_type":"markdown","metadata":{"id":"72kBlS8okKTb"},"source":["### <b> Model Training"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2930931,"status":"ok","timestamp":1690344169335,"user":{"displayName":"Mark Wan","userId":"11243616124484774426"},"user_tz":-480},"id":"f_4e5vwzhPDR","outputId":"811cf94d-ef81-47dc-8ace-54e9e797f7ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["step 0: train loss 4.3667, val loss 4.3769\n","step 500: train loss 2.0228, val loss 2.0944\n","step 1000: train loss 1.6118, val loss 1.7826\n","step 1500: train loss 1.4484, val loss 1.6460\n","step 2000: train loss 1.3521, val loss 1.5804\n","step 2500: train loss 1.2886, val loss 1.5329\n","step 3000: train loss 1.2356, val loss 1.5068\n","step 3500: train loss 1.1969, val loss 1.4936\n","step 4000: train loss 1.1571, val loss 1.4880\n","step 4500: train loss 1.1240, val loss 1.4755\n","step 4999: train loss 1.0880, val loss 1.4828\n"]}],"source":["# Define loss function and optimizer.\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","# Train model and return results.\n","# Note: If early stopping is False then patience is not used.\n","model = train_model(\n","                        train_data, val_data,\n","                        batch_size = batch_size,\n","                        context_size = context_size,\n","                        max_iterations = max_iterations,\n","                        eval_interval = eval_interval,\n","                        learning_rate = learning_rate,\n","                        eval_iters = eval_iters,\n","                        n_embeddings = n_embeddings,\n","                        n_heads = n_heads,\n","                        n_layers = n_layers,\n","                        dropout = dropout,\n","                        train_test_split = train_test_split,\n","                        model = model,\n","                        optimizer = optimizer,\n","                        device = device\n","                        )\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":1992,"status":"ok","timestamp":1690344171286,"user":{"displayName":"Mark Wan","userId":"11243616124484774426"},"user_tz":-480},"id":"nLMC691RIZdo"},"outputs":[],"source":["torch.save(model.state_dict(), \"./save_files/shakespeare_state.pt\")\n","\n","# model_scripted = torch.jit.script(model)\n","# model_scripted.save('./save_files/shakespeare_model.pt')"]},{"cell_type":"markdown","metadata":{"id":"j9oUndFmkKTc"},"source":["### <b> Generate Text"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41055,"status":"ok","timestamp":1690344212335,"user":{"displayName":"Mark Wan","userId":"11243616124484774426"},"user_tz":-480},"id":"vNdDRHk7dVc9","outputId":"58d05742-6792-43f0-b645-c2804b7a1bd0"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ANTIGONUS:\n","Is a vex to heaven to love it no dressengthing?\n","No, no. What flatter, for I chancel I play this?\n","\n","SOMERSET:\n","Caius, go like to qua need a folling eyes?\n","\n","TYBALT:\n","Gift to determining, for the fiery and myself,\n","Proceeds vengeance and enfaced intempts.\n","\n","QUEEN MARGARET:\n","O heavens and starts lives under their chites!\n","Beseech your party, sweet grows me weep\n","The faithful graces before me my when I'lder\n","Have strength my reedy health to thee.\n","Why then, or peach on hour my faith,\n","And fair offsame me accept in her?\n","Take her before soldiers me ne'er be flair,\n","And from their fearful tormed and them\n","To perpetter, having the born; thou art were.\n","O God, I stood blame aboard's frowness,\n","Dispartise meteor shinipable would have done\n","Thus I seem wy.\n","\n","FelloIthing Seesting have fruy to that hand.\n","O this safe? think and he number?\n","\n","ROMEO:\n","I know not to: let the many good how you behild!\n","\n","JULIET:\n","Nor Corioline that never it is a wupt forth--\n","As beggan an obscaved, as feed--he when\n","The sdanished their shouts on themselves now:\n","By afootish his brease out to get again.\n","\n","RICLIDam, good blesser:\n","That oursaluce Schorah's subject should be an earl,\n","Tyield their if formerly degreet stay.\n","Tell him when I now defends here away.\n","\n","BAGOT:\n","To her must we will away. Quickly Ely fear\n","Nowed wounted that I be gone these are little,\n","Who harived the crown with wing's sadness flees,\n","That I will spent by the before them\n","That deputy that my tongues many I love.\n","\n","ROMEO:\n","I retermist; hield, be understands, to again\n","With cornaction. The cheat haines wounded me!\n","I then will short not son of malaching.\n","Now Peter too, I'll reply to your brother's blood,\n","I cannot call him be homes behold, I lay\n","And mercy me from to be aliament.\n","Since hath been as Spirit acttains,\n","Communing Fortune, the words issued his thlown.\n","\n","GLOUCESTER:\n","Towards labours; the peoples is your learn'd\n","That certain big his might think to battle:\n","Come, let's and this lists at mercy far sying,\n","That as fifteen you more still between rest;\n","And to sp\n"]}],"source":["context = torch.zeros((1, 1), dtype=torch.long, device=device)\n","output = decode(model.generate(context, max_new_tokens=2000, context_size=context_size)[0].tolist())\n","print(output)\n","\n","with open('output_shakespeare.txt', 'w') a s f:\n","    f.write(output)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"GgoiKXOWN65b","executionInfo":{"status":"ok","timestamp":1690344212337,"user_tz":-480,"elapsed":21,"user":{"displayName":"Mark Wan","userId":"11243616124484774426"}}},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vscode":{"interpreter":{"hash":"ff95278ca4fdd316eea1c7e2e5a88b898011bc9033677a5a869983a754e9d9b2"}}},"nbformat":4,"nbformat_minor":0}